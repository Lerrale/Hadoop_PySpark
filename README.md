# Docker образ с HDFS и Spark для обучения и тренировки

Этот репозиторий содержит код для сборки Docker образа с HDFS и Spark, предназначенного для тренировочных целей. Включает в себя задачи с решениями, выполненные во время обучения в магистратуре МФТИ.


## Особенности
- Образ включает в себя предварительно настроенные HDFS, MapReduce и Spark для учебных и тренировочных целей на одной локальной машине.
- Включает задачи и решения по темам MapReduce, Spark DF, и Spark RDD. Подробные описания задач находятся в соответствующих директориях.
- В репозиторий приложена папка `tasks_solutions`, в которой собраны результаты выполнения задач.
- Папка с данными не добавлена в репозиторий из за большого объема. Ее можно скачать отдельно (ссылка ниже) и добавить в корневую директорию проекта.
- Можно воспроизвести решение задач, запустив их из контейнера.

  
## Структура проекта

- **tasks_solutions**: содержит результаты выполнения задач.
- **tasks**: директория с задачами на MapReduce, Spark DF, и Spark RDD. Условия задач находятся внутри.
- **hadoop-config**: конфигурационные файлы для Hadoop.
- **spark-config**: конфигурационные файлы для Spark.
- **Dockerfile**: Dockerfile для сборки Docker образа.
- **start-hadoop.sh**: скрипт для инициализации Hadoop и запуска служб NameNode, DataNode, ResourceManager и NodeManager.

## Сборка Docker образа

1) Чтобы собрать Docker образ необходимо:
   - скопировать репозиторий
   - скачать папку data и добавить ее в корень проекта:
      [ссылка для скачивания](https://drive.google.com/file/d/1wAVE3pnluh_uZOzgsH5QtpoUyfXzr8o_/view?usp=sharing)
   - запустить следующую команду из корня проекта:
```bash
docker build -t <имя_образа> .
```
2) Или скачать полностью готовый образ:
```bash
docker pull lerrale/hadoop_pyspark:latest
```
## Запуск задач
Задачи лежат в директории /usr/local/tasks/ .

Запуск решения каждой задачи должен происходить из папки с задачей в формате:
```bash
./run.sh <output_folder>
```






